{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "import random \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange, reduce\n",
    "from tqdm import tqdm, trange\n",
    "from torch.nn import functional as F\n",
    "from utils.download_mnist import mnist_dataloader_test\n",
    "from assembler import get_config, get_config_ebm, make_energy_model\n",
    "from utils.config import show \n",
    "from experiment import AdaptiveEBM\n",
    "\n",
    "path = !cd .. && pwd\n",
    "path = path[0]\n",
    "\n",
    "def plotable(img):\n",
    "    return rearrange(img, \"b c h w -> (b c h) w \").cpu().detach().numpy()\n",
    "\n",
    "def make_adaptive_experiment(config):\n",
    "    ebm = make_energy_model(config, path=path)\n",
    "    emb = ebm.to(\"cuda\")\n",
    "    adapt = AdaptiveEBM(config, ebm)\n",
    "    return adapt\n",
    "\n",
    "def get_model_config(model_name):\n",
    "    dataset, model, sampling, task = model_name.split(\"/\")\n",
    "    name = f\"{sampling}/{task}\"\n",
    "    config = get_config(get_config_ebm, dataset, model, name, path=path)\n",
    "    return config\n",
    "\n",
    "def pixelwise_varaince(imgs):\n",
    "    ex = reduce(imgs, \"b h w -> h w\", \"mean\")\n",
    "    ex2 = reduce(imgs**2, \"b h w -> h w\", \"mean\")\n",
    "    return ex2 - ex**2\n",
    "\n",
    "def reconstruction_error(x_hat, x, reduction=\"mean\"):\n",
    "    return F.mse_loss(x_hat, x, reduction=reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mingliang/anaconda3/envs/dgflowenv/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mnist/vae/langevin/inpainting\"\n",
    "config = get_model_config(model_name)\n",
    "dm = mnist_dataloader_test(config, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"exp_params\"][\"batch_size\"] = 1\n",
    "dm = mnist_dataloader_test(config, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iter(dm)\n",
    "x, y = next(gen)\n",
    "x = x.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"operator_params\"][\"size\"] = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_iteration(x):\n",
    "    adapt = make_adaptive_experiment(config)\n",
    "    x_tilde = adapt.ebm.operator(x)\n",
    "    avg_error = []\n",
    "    n_samples = 10\n",
    "    num_iter = 20\n",
    "    for i in range(num_iter):\n",
    "        imgs = torch.zeros(n_samples, 32, 32)\n",
    "        x_tilde = adapt.ebm.operator(x)\n",
    "        error = []\n",
    "        for num in range(0, n_samples):\n",
    "            x_hat = rearrange(adapt.ebm(x_tilde), \"b c h w -> (b c) h w\")\n",
    "            imgs[num, : , :] = x_hat\n",
    "            error.append(reconstruction_error(x, x_hat).detach().cpu())\n",
    "        avg_error.append(sum(error)/len(error))\n",
    "        A = adapt.adaptive_sample(imgs)\n",
    "        adapt.update_operator(A)\n",
    "        x_tilde = adapt.ebm.operator(x)\n",
    "    return np.array(avg_error, dtype=np.float32)\n",
    "\n",
    "def non_adaptive_iteration(x):\n",
    "    adapt = make_adaptive_experiment(config)\n",
    "    x_tilde = adapt.ebm.operator(x)\n",
    "    error = []\n",
    "    num_iter = 20\n",
    "    for _ in range(num_iter):\n",
    "        x_tilde = adapt.ebm.operator(x)\n",
    "        x_hat = rearrange(adapt.ebm(x_tilde), \"b c h w -> (b c) h w\")\n",
    "        error.append(reconstruction_error(x, x_hat).detach().cpu())\n",
    "        A = adapt.non_adaptive_sample(torch.zeros(1, 32, 32))\n",
    "        adapt.update_operator(A)\n",
    "        x_tilde = adapt.ebm.operator(x)\n",
    "    return np.array(error, dtype=np.float32)\n",
    "\n",
    "def epsilon_greedy_adaptive_iteration(x, epsilon=0.5):\n",
    "    adapt = make_adaptive_experiment(config)\n",
    "    x_tilde = adapt.ebm.operator(x)\n",
    "    error = []\n",
    "    num_iter = 20\n",
    "    n_samples = 10\n",
    "    for _ in range(num_iter):\n",
    "        if random.random()>epsilon:\n",
    "            imgs = torch.zeros(n_samples, 32, 32)\n",
    "            x_tilde = adapt.ebm.operator(x)\n",
    "            _error = []\n",
    "            for num in range(0, n_samples):\n",
    "                x_hat = rearrange(adapt.ebm(x_tilde), \"b c h w -> (b c) h w\")\n",
    "                imgs[num, : , :] = x_hat\n",
    "                _error.append(reconstruction_error(x, x_hat).detach().cpu())\n",
    "            error.append(sum(_error)/len(_error))\n",
    "            A = adapt.adaptive_sample(imgs)\n",
    "            adapt.update_operator(A)\n",
    "            x_tilde = adapt.ebm.operator(x)\n",
    "        else:\n",
    "            x_tilde = adapt.ebm.operator(x)\n",
    "            x_hat = rearrange(adapt.ebm(x_tilde), \"b c h w -> (b c) h w\")\n",
    "            error.append(reconstruction_error(x, x_hat).detach().cpu())\n",
    "            A = adapt.non_adaptive_sample(torch.zeros(1, 32, 32))\n",
    "            adapt.update_operator(A)\n",
    "            x_tilde = adapt.ebm.operator(x)\n",
    "        \n",
    "    return np.array(error, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99 [00:00<?, ?it/s]/home/mingliang/anaconda3/envs/dgflowenv/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: Using a target size (torch.Size([1, 32, 32])) that is different to the input size (torch.Size([1, 1, 32, 32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "/home/mingliang/anaconda3/envs/dgflowenv/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      " 89%|████████▉ | 88/99 [10:00<01:15,  6.84s/it]"
     ]
    }
   ],
   "source": [
    "avg_adaptive_error = np.zeros(20, dtype=np.float32)\n",
    "avg_adaptive_error_2 = np.zeros(20, dtype=np.float32)\n",
    "for t in trange(1, 100):\n",
    "    x, y = next(gen)\n",
    "    x = x.to(\"cuda\")\n",
    "    error = adaptive_iteration(x)\n",
    "    avg_adaptive_error = ((t-1)/t)*avg_adaptive_error + (1/t)*error\n",
    "    avg_adaptive_error_2 = ((t-1)/t)*avg_adaptive_error_2 + (1/t)*(error**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_error_non_adaptive = np.zeros(20, dtype=np.float32)\n",
    "avg_error_non_adaptive_2 = np.zeros(20, dtype=np.float32)\n",
    "for t in trange(1, 100):\n",
    "    x, y = next(gen)\n",
    "    x = x.to(\"cuda\")\n",
    "    error = non_adaptive_iteration(x)\n",
    "    avg_error_non_adaptive = ((t-1)/t)*avg_error_non_adaptive + (1/t)*error\n",
    "    avg_error_non_adaptive_2 = ((t-1)/t)*avg_error_non_adaptive_2 + (1/t)*error**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_error_e_greedy_adaptive = np.zeros(20, dtype=np.float32)\n",
    "avg_error_e_greedy_adaptive2 = np.zeros(20, dtype=np.float32)\n",
    "for t in trange(1, 100):\n",
    "    x, y = next(gen)\n",
    "    x = x.to(\"cuda\")\n",
    "    error = epsilon_greedy_adaptive_iteration(x)\n",
    "    avg_error_e_greedy_adaptive = ((t-1)/t)*avg_error_e_greedy_adaptive + (1/t)*error\n",
    "    avg_error_e_greedy_adaptive2 = ((t-1)/t)*avg_error_e_greedy_adaptive2 + (1/t)*error**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_adaptive = np.sqrt(avg_adaptive_error_2-avg_adaptive_error**2)\n",
    "std_non_adptive = np.sqrt(avg_error_non_adaptive_2 -avg_error_non_adaptive**2)\n",
    "std_adaptive = np.sqrt(avg_adaptive_error_2-avg_adaptive_error**2)\n",
    "std_non_adptive = np.sqrt(avg_error_non_adaptive_2 -avg_error_non_adaptive**2)\n",
    "std_e_greedy = np.sqrt(avg_error_e_greedy_adaptive2 -avg_error_e_greedy_adaptive**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Number of Iterations', fontsize=15)\n",
    "plt.ylabel('Reconstruction error (per pixel)', fontsize=15)\n",
    "plt.errorbar(list(range(1, 21)), avg_error_non_adaptive, yerr=std_non_adptive, fmt='-o')\n",
    "plt.errorbar(list(range(1, 21)), avg_adaptive_error, yerr=std_adaptive, fmt='-^')\n",
    "plt.errorbar(list(range(1, 21)), avg_error_e_greedy_adaptive, yerr=std_e_greedy, fmt='-s')\n",
    "plt.legend(['random sampling', 'adaptive', 'epsilon greedy sampling'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67ebdc3d7c5b8aee60e0976b3f0ba5a9f39f0ef6024b87b4abda2f48caa7c7ff"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('dgflowenv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
